{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e7c0a02",
   "metadata": {},
   "source": [
    "# Python Cheatsheets - DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8b011c",
   "metadata": {},
   "source": [
    "## Data Manipulation\n",
    "\n",
    "### Pandas\n",
    "Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool,\n",
    "built on top of the Python programming language. It is built on numpy and matplotlib libraries. It provides high-performance, easy-to-use data structures and data analysis tools for Python. \n",
    "\n",
    "More information: https://pandas.pydata.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6352ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN IF PANDAS IS NOT INSTALLED\n",
    "\n",
    "# install pandas either with conda or with pip\n",
    "# conda install -c conda-forge pandas\n",
    "%pip install pandas\n",
    "%pip install numerize\n",
    "\n",
    "# verify if it is installed\n",
    "# pip show pandas\n",
    "# pip show numerize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0adb20cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries for pandas and other necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from numerize.numerize import numerize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b069ac4a",
   "metadata": {},
   "source": [
    "#### 1) Data Classes\n",
    "\n",
    "Pandas has two types of classes to handle data: __Series__ and __DataFrame__\n",
    "\n",
    "- __Series:__ A one-dimensional labeled array holding data of any type such as integers, strings, Python objects etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ba3a1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:  <class 'pandas.core.series.Series'>\n",
      "0    1.0\n",
      "1    3.0\n",
      "2    4.0\n",
      "3    NaN\n",
      "4    6.0\n",
      "5    9.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# create a series by passing a list of values\n",
    "s = pd.Series([1, 3, 4, np.nan, 6, 9])\n",
    "print(\"Type: \",type(s), end=\"\\n\")\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedea56b",
   "metadata": {},
   "source": [
    "- __DataFrame:__ A two-dimensional data structure that holds data like a two-dimension array or a table with rows and columns.\n",
    "\n",
    "_DataFrame from a dictionary of lists:_ A DataFrame can be created by passing a _dictionary_ of lists where the keys are the labels and the values are the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da0e9c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:  <class 'pandas.core.frame.DataFrame'>\n",
      "           Country    Capital  Population\n",
      "DE         Germany     Berlin    84552242\n",
      "AU       Australia   Canberra    26713205\n",
      "JP           Japan      Tokyo   123753041\n",
      "IN           India  New Delhi  1450935791\n",
      "CN           China    Beijing  1419321278\n",
      "GB  United Kingdom     London    69138192\n"
     ]
    }
   ],
   "source": [
    "# create a DataFrame with a dictionary\n",
    "names = ['Germany', 'Australia', 'Japan', 'India', 'China', 'United Kingdom']\n",
    "cap =  ['Berlin', 'Canberra', 'Tokyo', 'New Delhi', 'Beijing', 'London']\n",
    "pop = [84552242, 26713205, 123753041, 1450935791, 1419321278, 69138192]\n",
    "codes = ['DE', 'AU', 'JP', 'IN', 'CN', 'GB']\n",
    "\n",
    "# create dictionary my_dict with three key:value pairs: my_dict\n",
    "my_dict = {\n",
    "    'Country':names,\n",
    "    'Capital':cap,\n",
    "    'Population':pop\n",
    "}\n",
    "\n",
    "# build a DataFrame countries from my_dict: countries\n",
    "countries = pd.DataFrame(my_dict)\n",
    "countries.index = codes\n",
    "\n",
    "# save the data as a csv file for the latter exercises\n",
    "countries.to_csv(\"data/countries.csv\")\n",
    "\n",
    "# print DataFrame\n",
    "print(\"Type: \",type(countries), end=\"\\n\")\n",
    "print(countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b0512c",
   "metadata": {},
   "source": [
    "_DataFrame from a list of dictionaries:_ A DataFrame can also be created by passing a _list_ of _dictionary_ objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1ee138a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date      sold\n",
      "0  2018-11-03  10376832\n",
      "1  2018-11-10  10717154\n"
     ]
    }
   ],
   "source": [
    "# create a list of dictionaries with new data\n",
    "orange_list = [\n",
    "    {\"date\": \"2018-11-03\", \"sold\": 10376832},\n",
    "    {\"date\": \"2018-11-10\", \"sold\": 10717154},\n",
    "]\n",
    "\n",
    "# convert list into DataFrame\n",
    "orange_sales = pd.DataFrame(orange_list)\n",
    "\n",
    "# print DataFrame\n",
    "print(orange_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e017d732",
   "metadata": {},
   "source": [
    "_DataFrame from a source file:_ Putting data in a dictionary and then building a DataFrame is not very efficient while dealing with millions of observations. A DataFrame can also be created by reading data from a _source file_ where the data is typically available with a regular structure. An example is the CSV file, which is short for \"comma-separated values\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fe14b33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Country    Capital  Population\n",
      "DE         Germany     Berlin    84552242\n",
      "AU       Australia   Canberra    26713205\n",
      "JP           Japan      Tokyo   123753041\n",
      "IN           India  New Delhi  1450935791\n",
      "CN           China    Beijing  1419321278\n",
      "GB  United Kingdom     London    69138192\n"
     ]
    }
   ],
   "source": [
    "# read the csv file (data extraxted from https://www.kaggle.com/datasets/lashagoch/life-expectancy-who-updated)\n",
    "countries = pd.read_csv('data/countries.csv', index_col=0)\n",
    "\n",
    "# print out the data\n",
    "print(countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aec7d4",
   "metadata": {},
   "source": [
    "#### 2) Exploring Data\n",
    "\n",
    "Pandas has several methods to explore a data and get a sense of its contents. \n",
    "\n",
    "__Methods:__\n",
    "- __head():__ It returns the first few rows of the DataFrame\n",
    "- __info():__ It displays the names of the columns, the data types they contain, and whether they have any missing values\n",
    "- __describe():__ It computes some summary statistics for numerical columns\n",
    "\n",
    "__Attributes:__\n",
    "- __shape:__ It contains a tuple that holds the number of rows followed by the number of columns\n",
    "- __values:__ It contains the data values in a 2D NumPy array\n",
    "- __columns:__ It contains the column names\n",
    "- __index:__ It contains row numbers or row names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1a70e49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (6, 3)\n",
      "Index: Index(['DE', 'AU', 'JP', 'IN', 'CN', 'GB'], dtype='object')\n",
      "Columns: Index(['Country', 'Capital', 'Population'], dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 6 entries, DE to GB\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Country     6 non-null      object\n",
      " 1   Capital     6 non-null      object\n",
      " 2   Population  6 non-null      int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 364.0+ bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# read the countries data\n",
    "#countries = pd.read_csv('data/countries.csv', index_col = 0)\n",
    "\n",
    "# check the shape of the DataFrame\n",
    "print(\"Shape:\", countries.shape)\n",
    "\n",
    "# check the index and columns of the DataFrame\n",
    "print(\"Index:\", countries.index)\n",
    "print(\"Columns:\", countries.columns)\n",
    "\n",
    "# explore the countries data with info\n",
    "print(countries.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec404e95",
   "metadata": {},
   "source": [
    "#### 3) Indexing and Selecting Data\n",
    "\n",
    "- __Square Brackets:__ The simplest, but not the most powerful way, to index and select is to use square brackets.\n",
    "\n",
    "    - Single square brackets ([]) return a Pandas Series.\n",
    "    - Double square brackets ([[]]) return a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e1997382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country column as a Pandas Series:\n",
      "DE           Germany\n",
      "AU         Australia\n",
      "JP             Japan\n",
      "IN             India\n",
      "CN             China\n",
      "GB    United Kingdom\n",
      "Name: Country, dtype: object\n",
      "\n",
      "Country and Population columns as a Pandas DataFrame:\n",
      "           Country  Population\n",
      "DE         Germany    84552242\n",
      "AU       Australia    26713205\n",
      "JP           Japan   123753041\n",
      "IN           India  1450935791\n",
      "CN           China  1419321278\n",
      "GB  United Kingdom    69138192\n"
     ]
    }
   ],
   "source": [
    "# read the countries data\n",
    "#countries = pd.read_csv('data/countries.csv', index_col = 0)\n",
    "\n",
    "# print out country column as Pandas Series\n",
    "print(\"Country column as a Pandas Series:\")\n",
    "print(countries['Country']) \n",
    "\n",
    "# print out country and population columns as Pandas DataFrame\n",
    "print(\"\\nCountry and Population columns as a Pandas DataFrame:\") \n",
    "print(countries[['Country', 'Population']]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811c96f2",
   "metadata": {},
   "source": [
    "- __Selecting Rows with Slicing:__ Use slices to select specific rows or observations. We can only select rows using square brackets if we specify a slice, like 0:4, using the integer indexes of the rows and not the row labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "be4e3856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Country   Capital  Population\n",
      "DE    Germany    Berlin    84552242\n",
      "AU  Australia  Canberra    26713205\n",
      "JP      Japan     Tokyo   123753041\n",
      "\n",
      "           Country    Capital  Population\n",
      "IN           India  New Delhi  1450935791\n",
      "CN           China    Beijing  1419321278\n",
      "GB  United Kingdom     London    69138192\n"
     ]
    }
   ],
   "source": [
    "# read the countries data\n",
    "#countries = pd.read_csv('data/countries.csv', index_col = 0)\n",
    "\n",
    "# print out first 3 observations\n",
    "print(countries[:3], end=\"\\n\\n\")\n",
    "\n",
    "# print out fourth, fifth and sixth observation\n",
    "print(countries[3:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a38ab4e",
   "metadata": {},
   "source": [
    "- ```loc``` __and__ ```iloc```__:__\n",
    "\n",
    "    - ```loc``` is label-based, using row and column labels.\n",
    "    - ```iloc``` is index-based, using integer positions.\n",
    "\n",
    "Pandas also allows to designate any column as an index. Setting a column as index is done with the ```.set_index()``` method while ```.reset_index()``` is used to reset the index to initial version. Setting the __drop__ parameter __True__ drops the newly set index column from the DataFrame. It is also possible to sort the DataFrame by index using the ```.sort_index()``` method. Setting the __level__ parameter defines the leves to sort by and __ascending__ parameters defines the type of the sorting to be done. The syntax is as below:\n",
    "```python\n",
    "df.set_index(\"col_A\")\n",
    "df.reset_index(drop=True)\n",
    "df.sort_index(level=[\"col_A\", \"col_B\"], ascending=[True, False])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b8f44cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country           Japan\n",
      "Capital           Tokyo\n",
      "Population    123753041\n",
      "Name: JP, dtype: object\n",
      "\n",
      "      Country   Capital  Population\n",
      "CN      China   Beijing  1419321278\n",
      "AU  Australia  Canberra    26713205\n",
      "\n",
      "loc['JP'] results the same as iloc[2]:  True\n",
      "loc[['AU','CN']] results the same as iloc[[1, 4]]:  True\n"
     ]
    }
   ],
   "source": [
    "# read the countries data\n",
    "#countries = pd.read_csv('data/countries.csv', index_col = 0)\n",
    "\n",
    "# print out observation for Japan as Pandas Series\n",
    "print(countries.loc['JP'], end=\"\\n\\n\")\n",
    "\n",
    "# print out observations for Australia and China as Pandas DataFrame\n",
    "print(countries.loc[['AU','CN']].sort_index(level=\"Capital\", ascending=False), end=\"\\n\\n\")\n",
    "\n",
    "# check loc and iloc results the same for Pandas Series\n",
    "print(\"loc['JP'] results the same as iloc[2]: \", countries.loc['JP'].equals(countries.iloc[2]))\n",
    "\n",
    "# check loc and iloc results the same for Pandas DataFrame\n",
    "print(\"loc[['AU','CN']] results the same as iloc[[1, 4]]: \", countries.loc[['AU','CN']].equals(countries.iloc[[1,4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14783a37",
   "metadata": {},
   "source": [
    "- __Combining Rows and Columns:__ Use ```loc``` and ```iloc``` to select specific rows and columns simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "690fb54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Population of India:  1450935791\n",
      "\n",
      "DE      84552242\n",
      "AU      26713205\n",
      "JP     123753041\n",
      "IN    1450935791\n",
      "CN    1419321278\n",
      "GB      69138192\n",
      "Name: Population, dtype: int64\n",
      "\n",
      "   Capital\n",
      "DE  Berlin\n",
      "\n",
      "           Country  Population\n",
      "IN           India  1450935791\n",
      "GB  United Kingdom    69138192\n",
      "\n",
      "loc['IN', 'Population'] results the same as iloc[3, 2]:  True\n",
      "loc[:,'Population'] results the same as iloc[:, 2]:  True\n",
      "loc[['IN','GB'],['Country','Population']] results the same as iloc[[3, 5], [0, 2]]:  True\n"
     ]
    }
   ],
   "source": [
    "# read the countries data\n",
    "#countries = pd.read_csv('data/countries.csv', index_col = 0)\n",
    "\n",
    "# print out Population value of India\n",
    "print(\"Population of India: \", countries.loc['IN', 'Population'], end=\"\\n\\n\")\n",
    "\n",
    "# print out Population column as Pandas Series\n",
    "print(countries.loc[:,'Population'], end=\"\\n\\n\")\n",
    "\n",
    "# print out Capital value of Germany as Pandas DataFrame\n",
    "print(countries.loc[['DE'],['Capital']], end=\"\\n\\n\")\n",
    "\n",
    "# print sub-DataFrame as Pandas DataFrame\n",
    "print(countries.loc[['IN','GB'],['Country','Population']], end=\"\\n\\n\")\n",
    "\n",
    "# check loc and iloc results the same\n",
    "print(\"loc['IN', 'Population'] results the same as iloc[3, 2]: \", countries.loc['IN', 'Population'] == countries.iloc[3, 2])\n",
    "\n",
    "# check loc and iloc results the same for Pandas Series\n",
    "print(\"loc[:,'Population'] results the same as iloc[:, 2]: \", countries.loc[:,'Population'].equals(countries.iloc[:, 2]))\n",
    "\n",
    "# check loc and iloc results the same for Pandas DataFrame\n",
    "print(\"loc[['IN','GB'],['Country','Population']] results the same as iloc[[3, 5], [0, 2]]: \", \n",
    "      countries.loc[['IN','GB'],['Country','Population']].equals(countries.iloc[[3, 5], [0, 2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a720634",
   "metadata": {},
   "source": [
    "#### 4) Sorting and Filtering Data\n",
    "\n",
    "Exploring and understanding data in a DataFrame is often easier with ordered rows. It is possible to sort the rows by passing a column name to ```.sort_values()```. To sort by multiple columns, a list of column names is passed to the function call.\n",
    "\n",
    "```python\n",
    "df.sort_values(\"column_1\")                                             # sorting by one column\n",
    "df.sort_values([\"column_1\", \"column_2\"])                               # sorting by multiple columns\n",
    "df.sort_values(\"column_1\", ascending = False)                          # sorting by one column in descending order\n",
    "df.sort_values([\"column_1\", \"column_2\"], ascending = [True, False])    # sorting by multiple columns in varying orders\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a7b78058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Country    Capital  Population\n",
      "GB  United Kingdom     London    69138192\n",
      "JP           Japan      Tokyo   123753041\n",
      "IN           India  New Delhi  1450935791\n",
      "DE         Germany     Berlin    84552242\n",
      "CN           China    Beijing  1419321278\n"
     ]
    }
   ],
   "source": [
    "# read the countries data\n",
    "#countries = pd.read_csv('data/countries.csv', index_col = 0)\n",
    "\n",
    "# sort countries by country names in descending order\n",
    "countries_sorted = countries.sort_values(\"Country\", ascending = False)\n",
    "\n",
    "# print sorted data\n",
    "print(countries_sorted.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6728a15",
   "metadata": {},
   "source": [
    "Comparison operators and NumPy logical operators are useful for filtering Pandas DataFrame by certain criteria. In order to filter, a boolean type of Pandas Series is necessary which can be obtained by using comparison and logical operators over DataFrame columns. To filter for multiple conditions at once, NumPy logical operators, ```np.logical_and()```, ```np.logical_or()``` etc., or \"bitwise\" operators, ```&```, ```|``` etc., are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f972d0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highly populated countries are: \n",
      "    Country    Capital  Population\n",
      "IN   India  New Delhi  1450935791\n",
      "CN   China    Beijing  1419321278\n",
      "\n",
      "Middle populated countries are: \n",
      "            Country Capital  Population\n",
      "DE         Germany  Berlin    84552242\n",
      "JP           Japan   Tokyo   123753041\n",
      "GB  United Kingdom  London    69138192\n"
     ]
    }
   ],
   "source": [
    "# read the countries data\n",
    "#countries = pd.read_csv('data/countries.csv', index_col = 0)\n",
    "\n",
    "# subset countries by population with comparison operators\n",
    "highly_pop = countries[countries['Population'] > 1000000000]\n",
    "\n",
    "# subset countries by population with numpy logical operators\n",
    "#middle_pop = countries[np.logical_and(countries['Population'] > 50000000, countries['Population'] < 1000000000)]\n",
    "\n",
    "# subset countries by population with bitwise logical operators\n",
    "middle_pop = countries[\n",
    "    (countries['Population'] > 50000000) & \n",
    "    (countries['Population'] < 1000000000)]\n",
    "\n",
    "# print subset data\n",
    "print(\"Highly populated countries are: \\n\", highly_pop)\n",
    "print(\"\\nMiddle populated countries are: \\n\", middle_pop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c73fba",
   "metadata": {},
   "source": [
    "Filtering data based on categorical variables often involves using the logical \"or\" operators, ```np.logical_or()``` or ```|```, to select rows from multiple categories. A more feasible and shorter way is to use the ```.isin()``` method, which allows writing one condition instead of separate ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "de7e65ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries with selected capitals are: \n",
      "     Country    Capital  Population\n",
      "DE  Germany     Berlin    84552242\n",
      "JP    Japan      Tokyo   123753041\n",
      "IN    India  New Delhi  1450935791\n"
     ]
    }
   ],
   "source": [
    "# define a list of desired capitals\n",
    "capitals = [\"Berlin\", \"Tokyo\", \"New Delhi\"]\n",
    "\n",
    "# filter the countries with isin call\n",
    "c_filtered = countries[countries[\"Capital\"].isin(capitals)]\n",
    "\n",
    "# print subset data\n",
    "print(\"Countries with selected capitals are: \\n\", c_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16386346",
   "metadata": {},
   "source": [
    "#### 5) Iterating Over Data\n",
    "\n",
    "Iterating over a Pandas DataFrame is typically done with the ```.iterrows()``` method. A for loop with ```.iterrows()``` iterates over every observation and on every iteration the row label and contents are available as:\n",
    "\n",
    "```python\n",
    "for lab, row in df.iterrows():\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0889f7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE: Germany\n",
      "AU: Australia\n",
      "JP: Japan\n",
      "IN: India\n",
      "CN: China\n",
      "GB: United Kingdom\n"
     ]
    }
   ],
   "source": [
    "# read the countries data\n",
    "#countries = pd.read_csv('data/countries.csv', index_col = 0)\n",
    "\n",
    "for lab, row in countries.iterrows() :\n",
    "    print(lab + \": \" + row[\"Country\"]) # also row.Country is suitable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7238e93e",
   "metadata": {},
   "source": [
    "However, iterating over a DataFrame results in creating a new Pandas Series at each iteration, which is not very efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d70096e",
   "metadata": {},
   "source": [
    "#### 6) Manipulating Data\n",
    "\n",
    "- __Add Column:__ Adding new columns to a DataFrame has many names, such as transforming, mutating, and feature engineering. It is possibe to generate new columns from scratch or by deriving them from existing columns. To add a column to a Pandas DataFrame by calling a function on another column ```.apply()``` call is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6e43768a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Country_Upper    Capital        Pop_M Pop_Readable\n",
      "DE         GERMANY     Berlin    84.552242       84.55M\n",
      "AU       AUSTRALIA   Canberra    26.713205       26.71M\n",
      "JP           JAPAN      Tokyo   123.753041      123.75M\n",
      "IN           INDIA  New Delhi  1450.935791        1.45B\n",
      "CN           CHINA    Beijing  1419.321278        1.42B\n",
      "GB  UNITED KINGDOM     London    69.138192       69.14M\n"
     ]
    }
   ],
   "source": [
    "# copy the countries DataFrame to avoid reflecting the changes on the original\n",
    "c_copy = countries.copy()\n",
    "\n",
    "# add a column with population in millions\n",
    "c_copy[\"Pop_M\"] = c_copy[\"Population\"] / 1000000\n",
    "\n",
    "# add a column with capitalized country names\n",
    "c_copy[\"Country_Upper\"] = c_copy[\"Country\"].apply(str.upper)\n",
    "\n",
    "# add a column with population in human readable format\n",
    "c_copy[\"Pop_Readable\"] = c_copy[\"Population\"].apply(numerize)\n",
    "\n",
    "# print the reulting DataFrame with new columns\n",
    "print(c_copy[[\"Country_Upper\", \"Capital\", \"Pop_M\", \"Pop_Readable\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e2f01c",
   "metadata": {},
   "source": [
    "- __Delete Column:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "73654146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Capital  Population   Country_Upper Pop_Readable\n",
      "DE     Berlin    84552242         GERMANY       84.55M\n",
      "AU   Canberra    26713205       AUSTRALIA       26.71M\n",
      "JP      Tokyo   123753041           JAPAN      123.75M\n",
      "IN  New Delhi  1450935791           INDIA        1.45B\n",
      "CN    Beijing  1419321278           CHINA        1.42B\n",
      "GB     London    69138192  UNITED KINGDOM       69.14M\n"
     ]
    }
   ],
   "source": [
    "d_copy = c_copy.drop(columns=[\"Pop_M\", \"Country\"])\n",
    "print(d_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4530d2f1",
   "metadata": {},
   "source": [
    "- __Rename Column:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a8dfcb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Country    Capital  Population Readable\n",
      "DE         GERMANY     Berlin    84552242   84.55M\n",
      "AU       AUSTRALIA   Canberra    26713205   26.71M\n",
      "JP           JAPAN      Tokyo   123753041  123.75M\n",
      "IN           INDIA  New Delhi  1450935791    1.45B\n",
      "CN           CHINA    Beijing  1419321278    1.42B\n",
      "GB  UNITED KINGDOM     London    69138192   69.14M\n"
     ]
    }
   ],
   "source": [
    "r_copy = d_copy.rename(columns={\"Country_Upper\": \"Country\", \"Pop_Readable\": \"Readable\"})[[\"Country\", \"Capital\", \"Population\", \"Readable\"]]\n",
    "print(r_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cf689d",
   "metadata": {},
   "source": [
    "- __Missing Values:__ Missing values need to be taken care of to avoid mistakes in analysing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0cebe222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country       False\n",
      "Capital       False\n",
      "Population    False\n",
      "Readable      False\n",
      "Life_Exp       True\n",
      "F_Life_Exp     True\n",
      "M_Life_Exp    False\n",
      "dtype: bool\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# add columns with some missing values\n",
    "r_copy[\"Life_Exp\"] = [81.54, 84.07, 84.85, np.nan, 78.02, 81.45]\n",
    "r_copy[\"F_Life_Exp\"] = [83.88, 85.85, 87.88, 73.86, np.nan, 83.33]\n",
    "r_copy[\"M_Life_Exp\"] = [79.21, 82.28, 81.83, 70.73, 75.25, 79.54]\n",
    "\n",
    "# check individual values for missing values\n",
    "#print(r_copy.isna())\n",
    "\n",
    "# check each column for missing values\n",
    "print(r_copy.isna().any())\n",
    "\n",
    "# bar plot of missing values by variable\n",
    "r_copy.isna().sum().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "17a748ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country       False\n",
      "Capital       False\n",
      "Population    False\n",
      "Readable      False\n",
      "Life_Exp      False\n",
      "F_Life_Exp    False\n",
      "M_Life_Exp    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# remove rows with missing values\n",
    "r_complete = r_copy.dropna()\n",
    "\n",
    "# check if any columns contain missing values\n",
    "print(r_complete.isna().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efc0c94",
   "metadata": {},
   "source": [
    "#### 7) Aggregating Data\n",
    "\n",
    "- __Summary Statistics:__ Summary statistics, such as mean, median, minimum, maximum, and standard deviation, allows to get a better sense of data. While Pandas and NumPy provide such many functions to summarize the data, the ```.agg()``` method allows applying custom functions as well as applying functions to more than one column of a DataFrame at once, making aggregations very efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93a15c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2864 entries, 0 to 2863\n",
      "Data columns (total 11 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   Country                    2864 non-null   object \n",
      " 1   Region                     2864 non-null   object \n",
      " 2   Year                       2864 non-null   int64  \n",
      " 3   Alcohol_consumption        2864 non-null   float64\n",
      " 4   BMI                        2864 non-null   float64\n",
      " 5   GDP_per_capita             2864 non-null   int64  \n",
      " 6   Population_mln             2864 non-null   float64\n",
      " 7   Schooling                  2864 non-null   float64\n",
      " 8   Economy_status_Developed   2864 non-null   int64  \n",
      " 9   Economy_status_Developing  2864 non-null   int64  \n",
      " 10  Life_expectancy            2864 non-null   float64\n",
      "dtypes: float64(5), int64(4), object(2)\n",
      "memory usage: 268.5+ KB\n",
      "None\n",
      "Mean BMI: 25.032925977653633\n",
      "Median BMI: 25.5\n",
      "Max Population (M): 1379.86\n",
      "Min Population (M): 0.08\n",
      "IQR of Population (M): 21.59\n"
     ]
    }
   ],
   "source": [
    "# read the csv file (data extraxted from https://www.kaggle.com/datasets/lashagoch/life-expectancy-who-updated)\n",
    "world = pd.read_csv('data/world_development.csv', index_col=0)\n",
    "\n",
    "# print the info about the World Development DataFrame\n",
    "print(world.info())\n",
    "\n",
    "# print the mean of BMI\n",
    "print(\"Mean BMI:\", world[\"BMI\"].mean())\n",
    "\n",
    "# print the median of BMI\n",
    "print(\"Median BMI:\", world[\"BMI\"].median())\n",
    "\n",
    "# print the maximum population in millions\n",
    "print(\"Max Population (M):\", world[\"Population_mln\"].max())\n",
    "\n",
    "# print the minimum population in millions\n",
    "print(\"Min Population (M):\", world[\"Population_mln\"].min())\n",
    "\n",
    "# define a custom IQR function\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "    \n",
    "# print IQR of the population in millions\n",
    "print(\"IQR of Population (M):\", world[\"Population_mln\"].agg(iqr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c93ef0",
   "metadata": {},
   "source": [
    "- __Counting and Grouping:__ Removing duplicates is essential to get accurate counts to avoid counting the same thing multiple times. ```.drop_duplicates()``` method allows to drop duplicates with respect to a defined subset. Counting allows to get an overview of data and to spot curiosities that might not be noticed otherwise. ```.value_counts()``` method allows to count numbers with respect to a column with specifying whther they should be normalized and sorted or not. The ```.groupby()``` method, on the other hand, allows to calculate grouped summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bae888a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Developing Countries in North America:\n",
      "328    Mexico\n",
      "Name: Country, dtype: object\n",
      "\n",
      "Proportion of Developing Countries in Each Region:\n",
      "Region\n",
      "Africa                           0.359155\n",
      "Asia                             0.183099\n",
      "Central America and Caribbean    0.133803\n",
      "Middle East                      0.091549\n",
      "South America                    0.084507\n",
      "Rest of Europe                   0.077465\n",
      "Oceania                          0.063380\n",
      "North America                    0.007042\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# subset the rows where the economy status is developing and drop duplicate countries\n",
    "developing_countries = world[world[\"Economy_status_Developing\"] == 1].drop_duplicates(subset=\"Country\")\n",
    "\n",
    "# print the developing countries in North America\n",
    "print(\"Developing Countries in North America:\")\n",
    "print(developing_countries[developing_countries[\"Region\"] == \"North America\"][\"Country\"])\n",
    "\n",
    "# get the proportion of developing countries in each region and sort\n",
    "print(\"\\nProportion of Developing Countries in Each Region:\")\n",
    "dev_props_sorted = developing_countries[\"Region\"].value_counts(sort=True, normalize=True)\n",
    "print(dev_props_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fb3feb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Proportion of Developing Countries Population by Region:\n",
      "Region\n",
      "Africa                           0.170869\n",
      "Asia                             0.631887\n",
      "Central America and Caribbean    0.014046\n",
      "Middle East                      0.049830\n",
      "North America                    0.018397\n",
      "Oceania                          0.001996\n",
      "Rest of Europe                   0.041256\n",
      "South America                    0.071720\n",
      "Name: Population_mln, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# group by region and get the sum of the population\n",
    "pop_by_region = developing_countries.groupby(\"Region\")[\"Population_mln\"].sum()\n",
    "\n",
    "# get proportion for each region\n",
    "print(\"\\nProportion of Developing Countries Population by Region:\")\n",
    "print(pop_by_region / sum(pop_by_region))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0c21e14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Schooling (avg years that people aged 25+ spent in formal education) Stats of Developing Countries by Region:\n",
      "                               min   max       mean  median\n",
      "Region                                                     \n",
      "Africa                         1.3   9.0   4.588235    4.00\n",
      "Asia                           2.2  11.7   7.100000    7.40\n",
      "Central America and Caribbean  4.3  11.0   7.836842    7.90\n",
      "Middle East                    2.8   9.6   7.761538    8.60\n",
      "North America                  7.0   7.0   7.000000    7.00\n",
      "Oceania                        4.3  10.3   6.811111    6.50\n",
      "Rest of Europe                 7.2  12.6  10.554545   10.80\n",
      "South America                  6.5  10.1   8.150000    8.05\n"
     ]
    }
   ],
   "source": [
    "# for each region in developing countries, aggregate schooling: get min, max, mean, and median\n",
    "print(\"\\nSchooling (avg years that people aged 25+ spent in formal education) Stats of Developing Countries by Region:\")\n",
    "schooling_stats = developing_countries.groupby(\"Region\")[\"Schooling\"].agg([\"min\", \"max\", \"mean\", \"median\"])\n",
    "print(schooling_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa03f9a",
   "metadata": {},
   "source": [
    "- __Pivot Tables:__ Pivot tables are the standard way of aggregating data in spreadsheets. In pandas, pivot tables are created with ```.pivot_table()``` method, and they are essentially another way of performing grouped calculations as an alternative to ```.groupby()```. The __aggfunc__ argument of ```.pivot_table()``` takes in a list of functions (without parentheses) that can be used to summarize the values. __fill_value__ replaces missing values with a substitute dummy value. __margins__ is a shortcut to pivot by two variables, while pivoting by each of those variables separately: it gives the row and column totals of the pivot table contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3a976928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean and Median Schooling (avg years that people aged 25+ spent in formal education) for each Region in Developing Countries:\n",
      "                                    mean    median\n",
      "                               Schooling Schooling\n",
      "Region                                            \n",
      "Africa                          4.588235      4.00\n",
      "Asia                            7.100000      7.40\n",
      "Central America and Caribbean   7.836842      7.90\n",
      "Middle East                     7.761538      8.60\n",
      "North America                   7.000000      7.00\n",
      "Oceania                         6.811111      6.50\n",
      "Rest of Europe                 10.554545     10.80\n",
      "South America                   8.150000      8.05\n"
     ]
    }
   ],
   "source": [
    "# pivot for mean and median schooling for each region in developing countries\n",
    "mean_med_schooling_by_region = developing_countries.pivot_table(values=\"Schooling\", index=\"Region\", aggfunc=[\"mean\", \"median\"])\n",
    "print(\"Mean and Median Schooling (avg years that people aged 25+ spent in formal education) for each Region in Developing Countries:\")\n",
    "print(mean_med_schooling_by_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "438bb5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schooling (avg years that people aged 25+ spent in formal education) for Developing Countries by Region and by Year:\n",
      "Year                               2003       2005       2007        All\n",
      "Region                                                                  \n",
      "Africa                         4.174510   4.358824   4.541176   4.358170\n",
      "Asia                           6.761538   6.961538   7.103846   6.942308\n",
      "Central America and Caribbean  7.384211   7.531579   7.731579   7.549123\n",
      "Middle East                    6.676923   6.938462   7.430769   7.015385\n",
      "North America                  7.100000   7.600000   8.000000   7.566667\n",
      "Oceania                        6.433333   6.577778   6.833333   6.614815\n",
      "Rest of Europe                 9.781818  10.063636  10.309091  10.051515\n",
      "South America                  7.625000   7.750000   7.800000   7.725000\n",
      "All                            6.196479   6.388028   6.593662   6.392723\n"
     ]
    }
   ],
   "source": [
    "# subset the rows where the economy status is developing for specified years\n",
    "years = [2003, 2005, 2007]\n",
    "developing_countries_by_years = world[(world[\"Economy_status_Developing\"] == 1) & (world[\"Year\"].isin(years))]\n",
    "\n",
    "# print the schooling in developing countries by region and years\n",
    "temp_dev_pivot = developing_countries_by_years.pivot_table(values=\"Schooling\", index=\"Region\", columns=\"Year\", fill_value=0, margins=True)\n",
    "print(\"Schooling (avg years that people aged 25+ spent in formal education) for Developing Countries by Region and by Year:\")\n",
    "print(temp_dev_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c302b2",
   "metadata": {},
   "source": [
    "The column 'All' returns an overall mean for each year, not (2003+2005+2007)/3. That would be a mean of means, rather than an overall mean per year! A pivot table is a DataFrame with sorted indexes, so the techniques to subset them are the same as DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "19c1f679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schooling (avg years that people aged 25+ spent in formal education) for Developing Countries from Africa to Middle East, and from 2003 to 2005:\n",
      "Year                               2003      2005\n",
      "Region                                           \n",
      "Africa                         4.174510  4.358824\n",
      "Asia                           6.761538  6.961538\n",
      "Central America and Caribbean  7.384211  7.531579\n",
      "Middle East                    6.676923  6.938462\n"
     ]
    }
   ],
   "source": [
    "# print the schooling in developing countries from Africa to Middle East, and from 2003 to 2005\n",
    "print(\"Schooling (avg years that people aged 25+ spent in formal education) for Developing Countries from Africa to Middle East, and from 2003 to 2005:\")\n",
    "print(temp_dev_pivot.loc[\"Africa\":\"Middle East\", 2003:2005])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617fd23b",
   "metadata": {},
   "source": [
    "Pivot tables are filled with summary statistics where further calculations can be performed on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f1d45eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region\n",
      "Africa    4.735095\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# pivot for mean schooling for each region and each year in developing countries\n",
    "schooling_by_region_year = developing_countries.pivot_table(values=\"Schooling\", index=\"Region\", columns=\"Year\")\n",
    "\n",
    "# get the mean temp by years\n",
    "mean_schooling_by_year = schooling_by_region_year.mean(axis=\"columns\")\n",
    "\n",
    "# Filter for the region that had the lowest mean schooling\n",
    "print(mean_schooling_by_year[mean_schooling_by_year == mean_schooling_by_year.min()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f83407",
   "metadata": {},
   "source": [
    "#### 8) Merging Data\n",
    "\n",
    "To merge two or more DatraFrames, ```.merge()``` call is used. It supports several join types like ```left```, ```right```, ```inner```, ```outer``` and ```cross```. __how__ param is used to specify the join type. By default ```.merge()``` uses inner join on columns that are present on both DataFrames. It is also possible to specify the columns to join and join by row index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "30b86180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframes from the dictionaries to observe various merge types\n",
    "emp_dict = {\n",
    "    'ID': ['0001', '0002', '0003', '0004', '0005', '0006'],\n",
    "    'Name': ['Mack Anthony', 'Lily Walker', 'Xander Quinn', 'Valery Ayers', 'Jaylen Ware', 'Oliver Mendoza'],\n",
    "    'DID': ['D001', 'D003', 'D001', 'D002', 'D006', 'D003'],\n",
    "    'PID': ['ASST', 'MAN', 'EX', 'MAN', 'EN', 'ASST'],\n",
    "}\n",
    "employees = pd.DataFrame(emp_dict)\n",
    "\n",
    "dept_dict = {\n",
    "    'DID': ['D001', 'D002', 'D003', 'D004'],\n",
    "    'DName': ['Sales', 'Admin', 'HR', 'Marketing'],\n",
    "    'DFloor': [2, 1, 4, 3],\n",
    "    'DLeadID': ['0003', '0004', '0002', '0008']\n",
    "}\n",
    "departments = pd.DataFrame(dept_dict)\n",
    "\n",
    "pos_dict = {\n",
    "    'PID': ['ASST', 'MAN', 'EN', 'EX'],\n",
    "    'Position': ['Assistant', 'Manager', 'Engineer', 'Executive'],\n",
    "    'Vacation': [20, 30, 30, 60],\n",
    "    'Salary' : [30000, 60000, 40000, 120000]\n",
    "}\n",
    "positions = pd.DataFrame(pos_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeed99a",
   "metadata": {},
   "source": [
    "- __Inner Join:__ Inner joins only return the rows with matching values in both tables. ```.merge()``` call is used without or with __how__ parameter set to __\"inner\"__. The column to merge on is specified with __on__ parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "80eeddc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DName\n",
      "Admin    1\n",
      "HR       2\n",
      "Sales    2\n",
      "Name: DName, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# merge the employees and departments tables\n",
    "emp_dept = employees.merge(departments, on=\"DID\")\n",
    "\n",
    "# print the number of people in each department\n",
    "print(emp_dept.groupby(\"DName\")[\"DName\"].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d2cca6",
   "metadata": {},
   "source": [
    "To merge more than two DataFrames, consecutive ```.merge()``` calls are used. A single line of code can be extended over multiple lines by ending it with a backslash as below (with no element or space ater the backslash):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e2f042f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Name  DName  DFloor DLeadID   Position  Vacation  Salary\n",
      "ID                                                                      \n",
      "0001    Mack Anthony  Sales       2    0003  Assistant        20   30000\n",
      "0002     Lily Walker     HR       4    0002    Manager        30   60000\n",
      "0003    Xander Quinn  Sales       2    0003  Executive        60  120000\n",
      "0004    Valery Ayers  Admin       1    0004    Manager        30   60000\n",
      "0006  Oliver Mendoza     HR       4    0002  Assistant        20   30000\n"
     ]
    }
   ],
   "source": [
    "# merge the employees and departments tables on DID; and merge the positions on PID\n",
    "emp_dept_pos = employees.merge(departments, on=\"DID\") \\\n",
    "    .merge(positions, on=\"PID\")\n",
    "    \n",
    "# print the emp_dept_pos without DID and PID\n",
    "print(emp_dept_pos.set_index('ID').drop(columns=['DID', 'PID']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c669a1aa",
   "metadata": {},
   "source": [
    "- __Left Join:__ A left (outer) join is useful to enhance or enrich a dataset, without losing any of the original data. Setting __how__ parameter __\"left\"__ with the ```.merge()``` method, it returns all of the rows of the left table, while using an inner join may result in lost data if it does not exist in both tables. The column to merge on is specified again with __on__ parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2ca076b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Name   DID   PID  DName  DFloor DLeadID\n",
      "ID                                                     \n",
      "0001    Mack Anthony  D001  ASST  Sales     2.0    0003\n",
      "0002     Lily Walker  D003   MAN     HR     4.0    0002\n",
      "0003    Xander Quinn  D001    EX  Sales     2.0    0003\n",
      "0004    Valery Ayers  D002   MAN  Admin     1.0    0004\n",
      "0005     Jaylen Ware  D006    EN    NaN     NaN     NaN\n",
      "0006  Oliver Mendoza  D003  ASST     HR     4.0    0002\n",
      "(6, 6)\n"
     ]
    }
   ],
   "source": [
    "# merge the employees and departments tables with a left join\n",
    "emp_dept = employees.merge(departments, on=\"DID\", how=\"left\").set_index('ID')\n",
    "\n",
    "# print the rows and shape of merged table\n",
    "print(emp_dept)\n",
    "print(emp_dept.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5543aa",
   "metadata": {},
   "source": [
    "- __Right Join:__ Setting __how__ parameter __\"right\"__ with the ```.merge()``` method performs a right (outer) join. It returns all of the rows of the right table, while only the rows with the keys in the left DataFrame that can be found in the right DataFrame will be displayed. The column to merge on is specified again with __on__ parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d1add433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Name   PID      DName  DFloor DLeadID\n",
      "ID                                                   \n",
      "0001    Mack Anthony  ASST      Sales       2    0003\n",
      "0003    Xander Quinn    EX      Sales       2    0003\n",
      "0004    Valery Ayers   MAN      Admin       1    0004\n",
      "0002     Lily Walker   MAN         HR       4    0002\n",
      "0006  Oliver Mendoza  ASST         HR       4    0002\n",
      "NaN              NaN   NaN  Marketing       3    0008\n"
     ]
    }
   ],
   "source": [
    "# merge the employees and departments tables with a right join\n",
    "emp_dept = employees.merge(departments, on=\"DID\", how=\"right\")\n",
    "\n",
    "# print the emp_dept\n",
    "print(emp_dept.set_index('ID').drop(columns=['DID']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7826fd",
   "metadata": {},
   "source": [
    "- __Outer Join:__ An outer join returns all rows from both merged tables and null where they do not match. It is useful to find rows that do not have a match in the other table. Setting __how__ parameter __\"outer\"__ with the ```.merge()``` method, it returns all of the rows of the both tables. The column to merge on is specified again with __on__ parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "16b14390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Name   DID PID DName  DFloor DLeadID\n",
      "ID                                               \n",
      "0005  Jaylen Ware  D006  EN   NaN     NaN     NaN\n"
     ]
    }
   ],
   "source": [
    "# merge the employees and departments tables with a right join\n",
    "emp_dept = employees.merge(departments, on=\"DID\", how=\"outer\")\n",
    "\n",
    "# create an index that returns true if DID or DLeadID are null\n",
    "missing = (emp_dept['DID'].isna() | emp_dept['DLeadID'].isna())\n",
    "\n",
    "# print the missing info on emp_dept\n",
    "print(emp_dept[missing].set_index('ID'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ae332d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-coursera",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
